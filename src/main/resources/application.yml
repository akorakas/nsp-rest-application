app:
  input:
    source: rest-api
#  kafka:
#    input-topic: snmp-traps      # ← REQUIRED for RequireTopics

  sinks:
    output:
      type: file
      file: C:\Users\akorakas\Desktop\output\output.ndjson
    dlt:
      type: kafka
      topic: snmp-traps.dlt
    error:
      type: kafka
      topic: snmp-traps.errors

  rest:
    nsp:
      scheme: https
      host: 172.17.52.133

      poll-interval-ms: 60000

      auth:
        basic: "bnNwcHJveHl1aTpLQGZrYVByb3ghYXRuMDFfbGFiOQ=="
        grant-type: "client_credentials"

      headers:
        content-type: "application/json"
        accept: "application/json"

      paths:
        token: "/rest-gateway/rest/api/v1/auth/token"
        alarms: "/FaultManagement/rest/api/v2/alarms/details"

spring:
  kafka:
    bootstrap-servers: 10.200.14.65:9092    # ← for AdminClient & default

    consumer:
      bootstrap-servers: 10.200.14.65:9092
      group-id: test09
      auto-offset-reset: earliest
      enable-auto-commit: false
      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      value-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      properties:
        "[client.id]": kafka-app-consumer
        "[spring.json.trusted.packages]": "com.example.kafka.model"
        "[spring.json.value.default.type]": "com.example.kafka.model.InputEvent"
        "[spring.json.use.type.headers]": false
        "[allow.auto.create.topics]": false
        "[max.poll.records]": 10000
        "[max.poll.interval.ms]": 600000
        "[default.api.timeout.ms]": 60000
        "[request.timeout.ms]": 30000
        "[fetch.min.bytes]": 1048576
        "[fetch.max.wait.ms]": 50
        "[partition.assignment.strategy]": org.apache.kafka.clients.consumer.CooperativeStickyAssignor
        "[security.protocol]": PLAINTEXT   # or your SASL_SSL config if you changed it

    producer:
      bootstrap-servers: 10.200.14.65:9092
      key-serializer: org.apache.kafka.common.serialization.StringSerializer
      value-serializer: org.apache.kafka.common.serialization.StringSerializer
      acks: all
      properties:
        "[client.id]": kafka-app-producer
        "[security.protocol]": PLAINTEXT   # again, adjust if you use SASL_SSL
        "[enable.idempotence]": true
        "[max.in.flight.requests.per.connection]": 5
        "[retries]": 2147483647
        "[linger.ms]": 5
        "[batch.size]": 65536
        "[compression.type]": lz4
        "[request.timeout.ms]": 5000
        "[max.block.ms]": 5000
        "[allow.auto.create.topics]": false

    listener:
      concurrency: 1
      ack-mode: BATCH
      auto-startup: false        # for your REST-driven app this is fine
      missing-topics-fatal: true

    admin:
      fail-fast: true
      properties:
        "[security.protocol]": PLAINTEXT
        "[request.timeout.ms]": 10000
        "[default.api.timeout.ms]": 10000

transform:
  placeholder: "{EVENT.RECOVERY.DATE} {EVENT.RECOVERY.TIME}"
  validate-on-start: true

  pipeline:
    - type: extract
      failOnMissing: true
      failOnBadJson: true
      mappings:
        emsVendorID: "/tags/mib"
        eventMessage: "/fields/event_message"
        eventNameRaw: "/fields/event_name"
        host: "/fields/host_name"
        resolvedRaw: "/fields/event_resolution_time"
        serialNoRaw: "/fields/event_id"
        severity: "/fields/severity"
        sourceEms: "/name"
        timestamp: "/timestamp"
        eventTimeRaw: "/fields/event_time"

    - type: update
      stripCr:
        - eventNameRaw
        - serialNoRaw
        - eventTimeRaw
        - resolvedRaw
      compute:
        - set: eventName
          expr: "eventNameRaw"
        - set: serialNo
          expr: "serialNoRaw"
        - set: resolvedAt
          expr: "resolvedRaw == placeholder ? '' : resolvedRaw"
        - set: Status
          expr: "resolvedAt == '' ? 'Open' : 'Closed'"

    - type: extract
      mappings:
        Community: "/tags/community"

    - type: regexExtract
      source: eventMessage
      pattern: "Created at: (.*?)\\r\\nResolved at:"
      group: 1
      target: createdAt
      fallback: eventTimeRaw

    - type: flatten
      roots: [ "fields", "tags" ]
      includeTop: [ "name", "timestamp" ]
      target: sourceEvent

    - type: hash
      algorithm: MD5
      fields: [ "host", "eventName", "serialNo", "timestamp" ]
      target: correlation_id

    - type: extract
      mappings:
        uptime: "/fields/sysUpTimeInstance"
        srcVersion: "/tags/version"
        oid: "/tags/oid"

    - type: extract
      fromVar: sourceEvent
      failOnBadJson: true
      mappings:
        mib: "/tags.mib"

    - type: template
      target: "$"
      template: |
        {
          "sourceEms": "${sourceEms}",
          "emsVendorID": "${emsVendorID}",
          "serialNo": "${serialNo}",
          "eventName": "${eventName}",
          "host": "${host}",
          "severity": "${severity}",
          "createdAt": "${createdAt}",
          "resolvedAt": "${resolvedAt}",
          "Status": "${Status}",
          "correlation_id": "${correlation_id}",
          "timestamp": ${timestamp},
          "uptime": ${uptime},
          "version": "${srcVersion}",
          "oid": "${oid}",
          "mib": "${mib}",
          "sourceEvent": ${sourceEvent},
          "Community": ["${Community}"]
        }
